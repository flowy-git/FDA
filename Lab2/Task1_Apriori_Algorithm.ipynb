{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Task 1 - Apriori Algorithm for Recommender System (33 points)\n",
    "\n",
    "**Task Definition:** In this programming assignment, you are required to implement the Apriori algorithm based on the lecture slides and apply it to mine frequent itemsets for recommendation. You are required to implement the algorithm from scratch by using only native Python libraries and NumPy. For efficiency you will need to convert the items to ids and sort them. Implement the algorithm based on the lecture slides and make use of native Python packages (such as collections and itertools).\n",
    "\n",
    "**Input:** The provided input file (`movies.txt`) based on the MovieLens dataset[1] contains the favourite movies of 1850 users. [1] Each line in the file corresponds to a user and represents a list of movies the user likes.\n",
    "\n",
    "An example:\n",
    "\n",
    "*Avengers: Infinity War - Part II;Jurassic World: Fallen Kingdom*\n",
    "\n",
    "In the example above, the corresponding user likes the movies \"Avengers: Infinity War - Part II\" and \"Jurassic World: Fallen Kingdom\".\n",
    "\n",
    "**Output:** You need to implement the Apriori algorithm and use it to mine frequent itemsets. Set the relative minimum support to 0.02 and run the algorithm on the 1850 list of movies. In other words, you need to extract all the itemsets that have an absolute support larger or equal to 37.\n",
    "\n",
    "[1] Harper, F.M., Konstan, J.A.: The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst. 5(4) (dec 2015). https://doi.org/10.1145/2827872, https://doi.org/10.1145/2827872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: uncomment the packages you used, please do not import additional non-native packages\n",
    "# You may change the imports to the following format: from [package] import [class, method, etc.]\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.1 Loading the data and preprocessing (3 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: read the data from the input file /data/movies.txt (1 points)\n",
    "records = []\n",
    "with open('data/movies.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        movies = line.strip().split(';')\n",
    "        records.append(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: determine the unique items and map the item to ids (1 points)\n",
    "unique_items = []\n",
    "\n",
    "for user_movies in records:\n",
    "    for movie in user_movies:\n",
    "        if movie not in unique_items:\n",
    "            unique_items.append(movie)\n",
    "\n",
    "id_to_item = unique_items\n",
    "item_to_id = {}\n",
    "\n",
    "for idx in range(len(id_to_item)):\n",
    "    item = id_to_item[idx]\n",
    "    item_to_id[item] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: map the items of the records to ids and sort each record (1 points)\n",
    "# In the following tasks use the mapped records to compute the frequent itemsets.\n",
    "sorted_records = []\n",
    "for movies in records:\n",
    "    movie_ids = sorted(item_to_id[movie] for movie in movies)\n",
    "    sorted_records.append(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.2 Apriori algorithm (21 points)\n",
    "### A) Prune the infrequent items (3 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1140, 3: 836, 7: 662, 2: 632, 0: 554, 24: 480, 8: 428, 4: 398, 30: 396, 10: 395, 9: 374, 6: 312, 12: 307, 19: 304, 11: 285, 16: 277, 27: 250, 33: 246, 13: 230, 21: 228, 37: 225, 28: 213, 48: 202, 17: 200, 25: 194, 23: 185, 29: 185, 14: 182, 15: 181, 22: 178, 38: 173, 47: 172, 5: 169, 26: 164, 32: 156, 18: 150, 31: 147, 40: 146, 20: 144, 42: 143, 34: 133, 43: 129, 36: 126, 45: 125, 41: 123, 44: 122, 35: 121, 46: 117, 39: 97, 49: 96})\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate the support of length-1 itemsets (1 points)\n",
    "\n",
    "itemsets = []\n",
    "for record in sorted_records:\n",
    "    itemsets.extend(record)\n",
    "support = collections.Counter(itemsets)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Store all frequent itemsets (keys) with their support (value) in the dictionary below.\n",
    "# TODO: add the length-1 frequent items with their supports to frequent_itemsets (2 points)\n",
    "frequent_itemsets = {}\n",
    "for item, support in support.items():\n",
    "    if support >= 37:\n",
    "        frequent_itemsets[frozenset({item})] = support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({0}): 554, frozenset({1}): 1140, frozenset({2}): 632, frozenset({3}): 836, frozenset({4}): 398, frozenset({5}): 169, frozenset({6}): 312, frozenset({7}): 662, frozenset({8}): 428, frozenset({9}): 374, frozenset({10}): 395, frozenset({11}): 285, frozenset({12}): 307, frozenset({13}): 230, frozenset({14}): 182, frozenset({15}): 181, frozenset({16}): 277, frozenset({17}): 200, frozenset({18}): 150, frozenset({19}): 304, frozenset({20}): 144, frozenset({21}): 228, frozenset({22}): 178, frozenset({23}): 185, frozenset({24}): 480, frozenset({25}): 194, frozenset({26}): 164, frozenset({27}): 250, frozenset({28}): 213, frozenset({29}): 185, frozenset({30}): 396, frozenset({31}): 147, frozenset({32}): 156, frozenset({33}): 246, frozenset({34}): 133, frozenset({35}): 121, frozenset({36}): 126, frozenset({37}): 225, frozenset({38}): 173, frozenset({39}): 97, frozenset({40}): 146, frozenset({41}): 123, frozenset({42}): 143, frozenset({43}): 129, frozenset({44}): 122, frozenset({45}): 125, frozenset({46}): 117, frozenset({47}): 172, frozenset({48}): 202, frozenset({49}): 96}\n"
     ]
    }
   ],
   "source": [
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### B) Determine the frequent n itemsets (15 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement the apriori_gen algorithm based on the lecture slides\n",
    "def apriori_gen(itemsets):\n",
    "    # TODO: generate candidates (4 points)\n",
    "    candidates = set()\n",
    "    itemsets_list = list(itemsets)\n",
    "\n",
    "    for i in range(len(itemsets_list)):\n",
    "        for j in range(i + 1, len(itemsets_list)):\n",
    "            set1 = itemsets_list[i]\n",
    "            set2 = itemsets_list[j]\n",
    "            union = set1.union(set2)\n",
    "            if len(union) == len(set1) + 1:\n",
    "                candidates.add(frozenset(union))\n",
    "\n",
    "    # TODO: prune the candidates and return them (4 points)\n",
    "    pruned_candidates = set()\n",
    "    for candidate in candidates:\n",
    "        valid = True\n",
    "        for subset in itertools.combinations(candidate, len(candidate)-1):\n",
    "            if frozenset(subset) not in itemsets:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            pruned_candidates.add(candidate)\n",
    "\n",
    "    toreturn = pruned_candidates\n",
    "    return toreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement an algorithm to calculate the support of the given itemset (2 points)\n",
    "# You do not need to implement a Hash Tree for calculating the supports.\n",
    "def calculate_support(itemset):\n",
    "    count = 0\n",
    "    for record in sorted_records:\n",
    "        record_set = set(record)\n",
    "        if itemset.issubset(record_set):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### note: the next cell can have a rather long runtime - not entirely sure if i missed an obvious optimisation step\n",
    "- last run it took >1min\n",
    "- has taken >5min at some point previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemset size being checked: 2\n",
      "itemset size being checked: 3\n",
      "itemset size being checked: 4\n",
      "itemset size being checked: 5\n",
      "itemset size being checked: 6\n",
      "itemset size being checked: 7\n",
      "itemset size being checked: 8\n",
      "itemset size being checked: 9\n",
      "itemset size being checked: 10\n",
      "ended with itemset size :  10\n"
     ]
    }
   ],
   "source": [
    "# TODO: set the initial frequent itemsets which needs to be the input of the first iteration (1 point)\n",
    "# (It will be updated at the end of each iteration.)\n",
    "frequent_n_itemsets = set(frequent_itemsets.keys())\n",
    "\n",
    "k = 2\n",
    "# TODO: set the correct loop condition until the Apriori algorithm should run (1 point)\n",
    "while frequent_n_itemsets:\n",
    "    print(\"itemset size being checked:\", k)\n",
    "    candidates = apriori_gen(frequent_n_itemsets)\n",
    "    if not candidates:\n",
    "        break\n",
    "    supports = [calculate_support(candidate) for candidate in candidates]\n",
    "\n",
    "    # TODO: filter out the frequent candidates (2 point)\n",
    "    frequent_candidates = {}\n",
    "    for candidate, support in zip(candidates, supports):\n",
    "        if support >= 37:\n",
    "            frequent_candidates[candidate] = support\n",
    "\n",
    "    # TODO: add the frequent candidates to frequent_itemsets (1 point)\n",
    "    frequent_itemsets.update(frequent_candidates)\n",
    "\n",
    "    # replace the frequent_n_itemsets for the next iteration\n",
    "    frequent_n_itemsets = set(frequent_candidates.keys())\n",
    "    k += 1\n",
    "print(\"ended with itemset size : \", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### C) Save your results (3 points)\n",
    "\n",
    "**Task:** Save all the frequent itemsets along with their absolute supports into a text file named `patterns.txt` and place it in the root of your zip file. Every line corresponds to exactly one frequent itemset and should be in the following format:\n",
    "\n",
    "*support:movie1;movie2;movie3;...*\n",
    "\n",
    "For example, suppose an itemset (Mission: Impossible - Fallout;A Star Is Born) has an absolute support 74, then the line corresponding to this frequent itemset in `patterns.txt` should be:\n",
    "\n",
    "*74:Mission: Impossible - Fallout;A Star Is Born*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open('patterns.txt', 'w') as f:\n",
    "        for itemset, support in frequent_itemsets.items():\n",
    "            movie_names = []\n",
    "            for item in itemset:\n",
    "                movie_names.append(id_to_item[item])\n",
    "            line = f\"{support}:{';'.join(movie_names)}\\n\"\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.3 Recommendation (9 points)\n",
    "\n",
    "**Task:** Imagine you should recommend a movie to a user. You know that the user likes the movies “Ant-Man and the Wasp” and “Spider-Man: Far from Home”. Based on the result of the Apriori algorithm, give a movie recommendation for this user by maximizing the confidence that the user will like the movie. Explain your choice and report the confidence score for your recommendation.(6 points)\n",
    "\n",
    "**Report:** Explain your method (comments in code or summary) and display your recommendations with the confidence scores. (3 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended movie:  Avengers: Infinity War - Part I\n",
      "\n",
      "Top matches with their support values:\n",
      "Avengers: Infinity War - Part I : 194102\n",
      "Deadpool 2 : 161593\n",
      "Avengers: Infinity War - Part II : 152658\n",
      "Spider-Man: Into the Spider-Verse : 126117\n",
      "Captain Marvel : 111672\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(user_likes):\n",
    "\n",
    "    itemsets = {}\n",
    "    with open(\"patterns.txt\", 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(':', 1)\n",
    "            support, movies = parts\n",
    "            movies = set(movies.split(';'))\n",
    "            itemsets[frozenset(movies)] = int(support)\n",
    "    \n",
    "    user_likes_set = set(user_likes)\n",
    "    recommendations = collections.defaultdict(int)\n",
    "    \n",
    "    for itemset, support in itemsets.items():\n",
    "        if user_likes_set & itemset:\n",
    "            for movie in itemset - user_likes_set:\n",
    "                recommendations[movie] += support\n",
    "    \n",
    "    sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_recs[:5]\n",
    "\n",
    "user_likes = [\"Ant-Man and the Wasp\", \"Spider-Man: Far from Home\"]\n",
    "recommendations = get_recommendations(user_likes)\n",
    "print(\"recommended movie: \", recommendations[0][0])\n",
    "print(\"\\nTop matches with their support values:\")\n",
    "for movie, score in recommendations:\n",
    "    print(movie, \":\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we load the patterns file that we previously calculated, which contains the frequent itemsets\n",
    "- we look at the intersection of the two movies the user likes (ant man & wasp, and spiderman FFH) with all the itemsets from the file\n",
    "- from those intersections, we then remove the two movies the user likes\n",
    "    - this is sort of like \"if $A\\cap B$ exists $\\implies$ take $B\\cap A^{\\complement}$\"\n",
    "    - these are \"people that liked your favourite movies also like these\"\n",
    "- we keep track of the movies in a dictionary, and we add up support values in the dictionary, which increases by increasing number of occurences of how often the movie is liked by someone who also likes user's favourite mvoies\n",
    "- in the end, we consider the top 5 movies -> ranked by the support score/value\n",
    "    - the top movie (with the highest support) is the one I recommend\n",
    "- essentially, I'm using the support as a \"confidence\" for recommending a movie?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

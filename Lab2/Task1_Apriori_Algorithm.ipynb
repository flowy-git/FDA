{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Task 1 - Apriori Algorithm for Recommender System (33 points)\n",
    "\n",
    "**Task Definition:** In this programming assignment, you are required to implement the Apriori algorithm based on the lecture slides and apply it to mine frequent itemsets for recommendation. You are required to implement the algorithm from scratch by using only native Python libraries and NumPy. For efficiency you will need to convert the items to ids and sort them. Implement the algorithm based on the lecture slides and make use of native Python packages (such as collections and itertools).\n",
    "\n",
    "**Input:** The provided input file (`movies.txt`) based on the MovieLens dataset[1] contains the favourite movies of 1850 users. [1] Each line in the file corresponds to a user and represents a list of movies the user likes.\n",
    "\n",
    "An example:\n",
    "\n",
    "*Avengers: Infinity War - Part II;Jurassic World: Fallen Kingdom*\n",
    "\n",
    "In the example above, the corresponding user likes the movies \"Avengers: Infinity War - Part II\" and \"Jurassic World: Fallen Kingdom\".\n",
    "\n",
    "**Output:** You need to implement the Apriori algorithm and use it to mine frequent itemsets. Set the relative minimum support to 0.02 and run the algorithm on the 1850 list of movies. In other words, you need to extract all the itemsets that have an absolute support larger or equal to 37.\n",
    "\n",
    "[1] Harper, F.M., Konstan, J.A.: The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst. 5(4) (dec 2015). https://doi.org/10.1145/2827872, https://doi.org/10.1145/2827872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: uncomment the packages you used, please do not import additional non-native packages\n",
    "# You may change the imports to the following format: from [package] import [class, method, etc.]\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.1 Loading the data and preprocessing (3 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: read the data from the input file /data/movies.txt (1 points)\n",
    "records = []\n",
    "with open('data/movies.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        movies = line.strip().split(';')\n",
    "        records.append(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: determine the unique items and map the item to ids (1 points)\n",
    "unique_items = []\n",
    "\n",
    "for user_movies in records:\n",
    "    for movie in user_movies:\n",
    "        if movie not in unique_items:\n",
    "            unique_items.append(movie)\n",
    "\n",
    "id_to_item = unique_items\n",
    "item_to_id = {}\n",
    "\n",
    "for idx in range(len(id_to_item)):\n",
    "    item = id_to_item[idx]\n",
    "    item_to_id[item] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: map the items of the records to ids and sort each record (1 points)\n",
    "# In the following tasks use the mapped records to compute the frequent itemsets.\n",
    "sorted_records = []\n",
    "for movies in records:\n",
    "    movie_ids = sorted(item_to_id[movie] for movie in movies)\n",
    "    sorted_records.append(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.2 Apriori algorithm (21 points)\n",
    "### A) Prune the infrequent items (3 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1140, 3: 836, 7: 662, 2: 632, 0: 554, 24: 480, 8: 428, 4: 398, 30: 396, 10: 395, 9: 374, 6: 312, 12: 307, 19: 304, 11: 285, 16: 277, 27: 250, 33: 246, 13: 230, 21: 228, 37: 225, 28: 213, 48: 202, 17: 200, 25: 194, 23: 185, 29: 185, 14: 182, 15: 181, 22: 178, 38: 173, 47: 172, 5: 169, 26: 164, 32: 156, 18: 150, 31: 147, 40: 146, 20: 144, 42: 143, 34: 133, 43: 129, 36: 126, 45: 125, 41: 123, 44: 122, 35: 121, 46: 117, 39: 97, 49: 96})\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate the support of length-1 itemsets (1 points)\n",
    "\n",
    "itemsets = []\n",
    "for record in sorted_records:\n",
    "    itemsets.extend(record)\n",
    "support = collections.Counter(itemsets)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Store all frequent itemsets (keys) with their support (value) in the dictionary below.\n",
    "# TODO: add the length-1 frequent items with their supports to frequent_itemsets (2 points)\n",
    "frequent_itemsets = {}\n",
    "for item, support in support.items():\n",
    "    frequent_itemsets[frozenset({item})] = support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({0}): 554, frozenset({1}): 1140, frozenset({2}): 632, frozenset({3}): 836, frozenset({4}): 398, frozenset({5}): 169, frozenset({6}): 312, frozenset({7}): 662, frozenset({8}): 428, frozenset({9}): 374, frozenset({10}): 395, frozenset({11}): 285, frozenset({12}): 307, frozenset({13}): 230, frozenset({14}): 182, frozenset({15}): 181, frozenset({16}): 277, frozenset({17}): 200, frozenset({18}): 150, frozenset({19}): 304, frozenset({20}): 144, frozenset({21}): 228, frozenset({22}): 178, frozenset({23}): 185, frozenset({24}): 480, frozenset({25}): 194, frozenset({26}): 164, frozenset({27}): 250, frozenset({28}): 213, frozenset({29}): 185, frozenset({30}): 396, frozenset({31}): 147, frozenset({32}): 156, frozenset({33}): 246, frozenset({34}): 133, frozenset({35}): 121, frozenset({36}): 126, frozenset({37}): 225, frozenset({38}): 173, frozenset({39}): 97, frozenset({40}): 146, frozenset({41}): 123, frozenset({42}): 143, frozenset({43}): 129, frozenset({44}): 122, frozenset({45}): 125, frozenset({46}): 117, frozenset({47}): 172, frozenset({48}): 202, frozenset({49}): 96}\n"
     ]
    }
   ],
   "source": [
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### B) Determine the frequent n itemsets (15 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement the apriori_gen algorithm based on the lecture slides\n",
    "def apriori_gen(itemsets):\n",
    "    # TODO: generate candidates (4 points)\n",
    "\n",
    "    # TODO: prune the candidates and return them (4 points)\n",
    "\n",
    "    toreturn = pruned_candidates\n",
    "    return toreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement an algorithm to calculate the support of the given itemset (2 points)\n",
    "# You do not need to implement a Hash Tree for calculating the supports.\n",
    "def calculate_support(itemset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: set the initial frequent itemsets which needs to be the input of the first iteration (1 point)\n",
    "# (It will be updated at the end of each iteration.)\n",
    "frequent_n_itemsets = None\n",
    "\n",
    "# TODO: set the correct loop condition until the Apriori algorithm should run (1 point)\n",
    "while False:\n",
    "    candidates = apriori_gen(frequent_n_itemsets)\n",
    "    supports = map(calculate_support, candidates)\n",
    "\n",
    "    # TODO: filter out the frequent candidates (2 point)\n",
    "    frequent_candidates = {}\n",
    "\n",
    "    # TODO: add the frequent candidates to frequent_itemsets (1 point)\n",
    "    pass\n",
    "\n",
    "    # replace the frequent_n_itemsets for the next iteration\n",
    "    frequent_n_itemsets = [itemset for itemset in frequent_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### C) Save your results (3 points)\n",
    "\n",
    "**Task:** Save all the frequent itemsets along with their absolute supports into a text file named `patterns.txt` and place it in the root of your zip file. Every line corresponds to exactly one frequent itemset and should be in the following format:\n",
    "\n",
    "*support:movie1;movie2;movie3;...*\n",
    "\n",
    "For example, suppose an itemset (Mission: Impossible - Fallout;A Star Is Born) has an absolute support 74, then the line corresponding to this frequent itemset in `patterns.txt` should be:\n",
    "\n",
    "*74:Mission: Impossible - Fallout;A Star Is Born*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.3 Recommendation (9 points)\n",
    "\n",
    "**Task:** Imagine you should recommend a movie to a user. You know that the user likes the movies “Ant-Man and the Wasp” and “Spider-Man: Far from Home”. Based on the result of the Apriori algorithm, give a movie recommendation for this user by maximizing the confidence that the user will like the movie. Explain your choice and report the confidence score for your recommendation.(6 points)\n",
    "\n",
    "**Report:** Explain your method (comments in code or summary) and display your recommendations with the confidence scores. (3 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
